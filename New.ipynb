{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>jens</th>\n",
       "      <th>dard ghafase sineh</th>\n",
       "      <th>feshar khun dar halat esterahat</th>\n",
       "      <th>kolestrol</th>\n",
       "      <th>ghand khun nashta</th>\n",
       "      <th>navar ghalb dar halat esterahat</th>\n",
       "      <th>hadaksar zaraban ghalb</th>\n",
       "      <th>anjin sadri nashi az varzesh</th>\n",
       "      <th>afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat</th>\n",
       "      <th>shibe tamrin dar oje tamrin dar maghtae ST</th>\n",
       "      <th>tedad oroghe bozorg rangi ba flourosopy</th>\n",
       "      <th>talasemi</th>\n",
       "      <th>ehtemal voghu bimari ghalbi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sen  jens  dard ghafase sineh  feshar khun dar halat esterahat  kolestrol  \\\n",
       "0   63     1                   1                              145        233   \n",
       "1   67     1                   4                              160        286   \n",
       "2   67     1                   4                              120        229   \n",
       "3   37     1                   3                              130        250   \n",
       "4   41     0                   2                              130        204   \n",
       "\n",
       "   ghand khun nashta  navar ghalb dar halat esterahat  hadaksar zaraban ghalb  \\\n",
       "0                  1                                2                     150   \n",
       "1                  0                                2                     108   \n",
       "2                  0                                2                     129   \n",
       "3                  0                                0                     187   \n",
       "4                  0                                2                     172   \n",
       "\n",
       "   anjin sadri nashi az varzesh  \\\n",
       "0                             0   \n",
       "1                             1   \n",
       "2                             1   \n",
       "3                             0   \n",
       "4                             0   \n",
       "\n",
       "   afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat  \\\n",
       "0                                                2.3                   \n",
       "1                                                1.5                   \n",
       "2                                                2.6                   \n",
       "3                                                3.5                   \n",
       "4                                                1.4                   \n",
       "\n",
       "   shibe tamrin dar oje tamrin dar maghtae ST  \\\n",
       "0                                           3   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "3                                           3   \n",
       "4                                           1   \n",
       "\n",
       "    tedad oroghe bozorg rangi ba flourosopy  talasemi  \\\n",
       "0                                         0         6   \n",
       "1                                         3         3   \n",
       "2                                         2         7   \n",
       "3                                         0         3   \n",
       "4                                         0         3   \n",
       "\n",
       "   ehtemal voghu bimari ghalbi  \n",
       "0                            0  \n",
       "1                            1  \n",
       "2                            1  \n",
       "3                            0  \n",
       "4                            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "Health=pd.read_excel('Health.xlsx')\n",
    "Health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sen</th>\n",
       "      <th>jens</th>\n",
       "      <th>dard ghafase sineh</th>\n",
       "      <th>feshar khun dar halat esterahat</th>\n",
       "      <th>kolestrol</th>\n",
       "      <th>ghand khun nashta</th>\n",
       "      <th>navar ghalb dar halat esterahat</th>\n",
       "      <th>hadaksar zaraban ghalb</th>\n",
       "      <th>anjin sadri nashi az varzesh</th>\n",
       "      <th>afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat</th>\n",
       "      <th>shibe tamrin dar oje tamrin dar maghtae ST</th>\n",
       "      <th>tedad oroghe bozorg rangi ba flourosopy</th>\n",
       "      <th>talasemi</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sen  jens  dard ghafase sineh  feshar khun dar halat esterahat  kolestrol  \\\n",
       "0   63     1                   1                              145        233   \n",
       "1   67     1                   4                              160        286   \n",
       "2   67     1                   4                              120        229   \n",
       "\n",
       "   ghand khun nashta  navar ghalb dar halat esterahat  hadaksar zaraban ghalb  \\\n",
       "0                  1                                2                     150   \n",
       "1                  0                                2                     108   \n",
       "2                  0                                2                     129   \n",
       "\n",
       "   anjin sadri nashi az varzesh  \\\n",
       "0                             0   \n",
       "1                             1   \n",
       "2                             1   \n",
       "\n",
       "   afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat  \\\n",
       "0                                                2.3                   \n",
       "1                                                1.5                   \n",
       "2                                                2.6                   \n",
       "\n",
       "   shibe tamrin dar oje tamrin dar maghtae ST  \\\n",
       "0                                           3   \n",
       "1                                           2   \n",
       "2                                           2   \n",
       "\n",
       "    tedad oroghe bozorg rangi ba flourosopy  talasemi  Outcome  \n",
       "0                                         0         6        0  \n",
       "1                                         3         3        1  \n",
       "2                                         2         7        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Health=Health.rename(columns={'ehtemal voghu bimari ghalbi':'Outcome'})\n",
    "Health.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "X=Health[['sen','jens','dard ghafase sineh','feshar khun dar halat esterahat','kolestrol','ghand khun nashta','navar ghalb dar halat esterahat','hadaksar zaraban ghalb','anjin sadri nashi az varzesh','afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat','shibe tamrin dar oje tamrin dar maghtae ST','talasemi']]\n",
    "y=Health['Outcome']\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38828936  1.33406684  3.44327815 ...  1.29219224  0.38779992\n",
      "  -0.50802052]\n",
      " [ 2.53157665  1.26990087 -0.63439288 ... -0.91931425  1.02500587\n",
      "   1.04124644]\n",
      " [ 2.82733451 -0.68857645 -0.37631964 ...  0.17049075  0.06364086\n",
      "   0.13978765]\n",
      " ...\n",
      " [ 2.21376223  0.05447609  2.11303092 ...  0.52054735 -1.60830955\n",
      "   0.99242719]\n",
      " [ 1.94325279 -2.30961818 -0.08870888 ... -1.08001929  0.25768564\n",
      "  -0.50327746]\n",
      " [-1.51995365  1.37480367 -0.13637849 ... -0.2395704   0.13751982\n",
      "  -0.93371892]]\n",
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      0\n",
      "4      0\n",
      "5      0\n",
      "6      1\n",
      "7      0\n",
      "8      1\n",
      "9      1\n",
      "10     0\n",
      "11     0\n",
      "12     1\n",
      "13     0\n",
      "14     0\n",
      "15     0\n",
      "16     1\n",
      "17     0\n",
      "18     0\n",
      "19     0\n",
      "20     0\n",
      "21     0\n",
      "22     1\n",
      "23     1\n",
      "24     1\n",
      "25     0\n",
      "26     0\n",
      "27     0\n",
      "28     0\n",
      "29     1\n",
      "      ..\n",
      "267    0\n",
      "268    1\n",
      "269    0\n",
      "270    1\n",
      "271    0\n",
      "272    0\n",
      "273    0\n",
      "274    1\n",
      "275    0\n",
      "276    1\n",
      "277    0\n",
      "278    1\n",
      "279    0\n",
      "280    1\n",
      "281    1\n",
      "282    1\n",
      "283    0\n",
      "284    0\n",
      "285    1\n",
      "286    0\n",
      "287    1\n",
      "288    1\n",
      "289    1\n",
      "290    0\n",
      "291    1\n",
      "292    1\n",
      "293    1\n",
      "294    1\n",
      "295    1\n",
      "296    1\n",
      "Name: Outcome, Length: 297, dtype: int64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 1/150, cost: 0.6164246765910629\n",
      "INFO:pyswarms.single.global_best:Iteration 2/150, cost: 0.6103800047537332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0    0.359043 -0.478038 -1.196012 -1.350938  0.451145  1.910035 -1.245424   \n",
      "1    0.621705 -0.383330  1.087039 -2.756279  0.510644  1.135158  1.597337   \n",
      "2   -1.748255  0.807710 -0.508229  1.358312  0.063417 -0.757197  0.522604   \n",
      "3    0.178695  0.269357  0.038703  0.841490 -1.556206  0.833097 -0.211277   \n",
      "4   -0.756690 -0.046718 -0.361287 -1.975666 -0.983201  0.157030  0.358294   \n",
      "5    0.693285  1.665160 -0.775157  1.818460  1.363071  0.966014 -1.618129   \n",
      "6    1.426578 -0.149783 -1.396302  0.035249  1.406922 -0.649799  0.421426   \n",
      "7    1.120785 -1.091626 -0.503981 -1.269311  1.006655 -1.021362 -0.341310   \n",
      "8   -0.988331 -0.275733  1.221033 -0.881730 -0.639467  0.049358 -1.431117   \n",
      "9   -0.702837 -1.503357  0.932560  0.909149  0.207725 -0.506145 -0.345953   \n",
      "10   1.069172 -0.932002 -1.486574 -0.560332 -0.967370  1.448769  0.332442   \n",
      "11   0.060604  0.320396 -1.447643 -0.038300 -0.790805  0.826968  1.287852   \n",
      "12   2.746877 -0.900467 -0.669359 -0.606998 -0.689519  0.480282 -0.208615   \n",
      "13  -1.444837 -0.217267 -1.223314  1.412697  0.197420 -0.002516  0.881754   \n",
      "14  -0.485868  4.005398 -0.035141 -1.571468  0.232613  0.215345  1.260188   \n",
      "15   2.260981  2.672860 -0.882356  0.014177 -1.670982 -1.919460  0.535257   \n",
      "16  -0.905398 -1.172967  0.106558  0.290326 -0.203595  1.976091 -1.215036   \n",
      "17  -0.931783  1.661714  2.452030 -1.504139 -1.249959 -2.196621 -0.871337   \n",
      "18  -0.344403  0.261664  3.014074 -0.844112  1.891685 -0.765202  0.808942   \n",
      "19  -1.198919 -0.417791  0.955239  0.686363  0.645033 -0.610687 -0.751140   \n",
      "20   2.288699  0.107012 -0.414311 -1.112795 -0.267216  0.226303 -0.528334   \n",
      "21   0.335345  1.209757  2.484741  0.438601 -0.744813 -0.805790 -1.898108   \n",
      "22   1.396558  0.173626  0.354254  0.273673 -0.565825  0.474284 -1.806072   \n",
      "23   0.456413 -1.920096  0.576483  1.177926  0.204977 -0.435142 -0.304285   \n",
      "24   1.974200 -0.441822 -0.435776 -0.311143  0.486276 -1.218671 -0.415773   \n",
      "25  -3.076938 -1.243825  1.251749  0.251976 -1.694492  0.923549 -0.235573   \n",
      "26   0.002035 -1.359016 -0.038360 -0.475158  1.702989  0.001402 -1.540724   \n",
      "27   1.358261  0.229968  1.456473 -1.237472  0.309290  1.389082  1.218878   \n",
      "28  -2.597423 -1.342597 -0.242338 -0.397396 -0.226696 -0.376886  0.387188   \n",
      "29   2.595420 -0.332101 -0.407769  1.412691 -1.446428 -0.643607  0.314640   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "180  1.155026 -3.004258 -0.414260  0.136926  0.016292 -0.341512  1.172076   \n",
      "181 -0.100370 -0.661019  1.822408  1.122409 -1.403970  0.225904 -1.197200   \n",
      "182  1.403840  1.298340 -0.216201  0.590834 -0.852827 -0.626226  0.141317   \n",
      "183  2.691471 -1.186856 -1.069202 -0.186387  1.141094 -0.310901 -0.554562   \n",
      "184  2.161997 -2.260793 -0.172533  0.091744 -0.931703  0.994186  0.344931   \n",
      "185 -0.169800 -1.685147  0.918298  1.587570 -0.724572 -0.752253  0.220377   \n",
      "186  1.003416 -0.291216  1.819282  0.700065  1.605158  0.535537  0.766549   \n",
      "187  2.009375  0.258119  0.169936 -1.430199 -0.840446 -0.715944 -0.002168   \n",
      "188  1.256106 -0.127344 -1.654737 -0.156850  0.389379 -1.344328  1.326334   \n",
      "189 -2.427356 -0.593721 -0.015278 -0.309620 -0.287391 -0.796139  0.003970   \n",
      "190  0.513863  0.325194  2.006222  1.142955 -0.262392  2.245529  1.281436   \n",
      "191 -0.160706  1.329487 -0.908878 -0.459436  1.359832 -1.493842 -0.764733   \n",
      "192 -0.614671  2.273868 -0.202795 -0.630191  1.610182  2.541296  0.099877   \n",
      "193 -2.023788  1.364167 -1.589027 -0.348033  0.426870 -1.024566  0.255263   \n",
      "194 -2.792980 -0.681420 -0.960698  0.583284  0.193458 -0.204303  1.091533   \n",
      "195  0.353275 -0.394382  0.695202 -0.242671 -0.776290  0.344571 -0.997895   \n",
      "196  1.767456 -1.178448 -0.588164 -0.908167 -0.631263  0.785858 -0.276423   \n",
      "197  2.557146 -0.967052 -0.038972 -0.241179 -0.493490  0.560181 -0.455878   \n",
      "198  2.608986  1.947937  2.288796 -0.964902  0.553959 -0.015786  0.992928   \n",
      "199  2.798990 -0.664467 -0.313394 -0.408071 -0.915048  0.116702 -0.134915   \n",
      "200  3.471429  2.022934  0.911916  0.093981 -0.334290 -0.327632  2.975697   \n",
      "201  0.074294 -1.439320 -0.477455 -1.814837  0.701711 -1.055556  0.187956   \n",
      "202  2.529481 -1.271096 -0.573137  0.214056  0.199101 -1.035348  0.211388   \n",
      "203  1.161327 -0.204267 -0.227247 -0.778984 -0.220556  1.834331  1.100136   \n",
      "204 -0.503584 -1.093946 -0.289106 -1.139314 -0.440683  0.270554 -0.370268   \n",
      "205  2.263775  0.512814  2.049042  0.132647  1.646540  1.476416  0.038827   \n",
      "206  1.995648 -0.556224 -0.477834 -0.360014  0.411651 -1.265777 -0.206961   \n",
      "207  2.472680 -0.735132 -0.118080  0.493027 -0.460560  1.023765 -0.477065   \n",
      "208  1.312780  0.249613  1.175412 -1.132524  0.300986  1.627705  1.354277   \n",
      "209  2.084072  0.872706  0.063946 -0.796030 -0.406569  0.620809  2.991098   \n",
      "\n",
      "            7         8         9  \n",
      "0   -0.442059  0.795145  0.680877  \n",
      "1   -0.135126  0.233293 -0.568305  \n",
      "2    0.199577  0.513068 -0.771225  \n",
      "3    0.803394 -0.079428  0.027770  \n",
      "4   -1.450162  0.582158  0.948050  \n",
      "5   -0.412969 -0.250912 -0.207460  \n",
      "6   -0.811319  0.361069 -1.597800  \n",
      "7   -0.168226  0.310785  0.331247  \n",
      "8    0.011266 -0.166807 -1.097146  \n",
      "9    0.040877  0.392886 -1.245002  \n",
      "10   0.330899  1.076998  0.425277  \n",
      "11  -1.690404  0.604489 -0.615480  \n",
      "12   0.238315  0.421142 -0.310774  \n",
      "13   0.230586 -0.075655 -0.646532  \n",
      "14   1.655164 -0.079452  0.409408  \n",
      "15   1.032938 -1.468049  0.209548  \n",
      "16  -0.682220 -0.136369  0.258482  \n",
      "17  -1.751362 -0.257778 -0.620310  \n",
      "18  -1.020406 -0.782930 -0.169076  \n",
      "19  -1.337775 -0.434871  1.499541  \n",
      "20  -0.728593  0.678544 -0.471359  \n",
      "21  -0.637920  0.421028 -1.230584  \n",
      "22   1.710104  0.285751 -1.106366  \n",
      "23   0.498997 -0.678007 -0.373970  \n",
      "24   1.236815  1.101001 -0.034249  \n",
      "25  -0.397162  0.967479 -0.450604  \n",
      "26   0.041416 -1.278810 -0.196026  \n",
      "27   0.521185  0.948080 -0.501848  \n",
      "28   0.439471 -0.047359  0.650299  \n",
      "29   2.160535 -1.196104  1.076989  \n",
      "..        ...       ...       ...  \n",
      "180 -0.460162  0.397029 -0.514616  \n",
      "181  0.728127  0.221054 -0.982529  \n",
      "182 -0.643962 -1.722441 -0.618232  \n",
      "183  0.857047  0.638434 -0.109781  \n",
      "184 -0.738606  0.154961 -0.441626  \n",
      "185  1.558498 -0.111510 -1.116426  \n",
      "186  1.383248 -0.573367  0.349310  \n",
      "187 -1.056917  0.302657 -0.456221  \n",
      "188  0.079475  0.318762 -1.080715  \n",
      "189  0.941681  0.816409  0.363931  \n",
      "190  0.535014 -0.392857  0.314958  \n",
      "191  0.562009 -0.429093 -1.296712  \n",
      "192  1.350758 -0.205268 -0.339177  \n",
      "193  1.007943 -0.055826  0.090124  \n",
      "194 -0.613765 -0.224299 -0.466200  \n",
      "195  0.291923 -0.726504 -1.034505  \n",
      "196  0.333626  0.148235 -0.473338  \n",
      "197 -0.568594  0.040653 -0.041353  \n",
      "198 -0.978621  0.453567 -0.034637  \n",
      "199  0.049991  0.153128  0.224461  \n",
      "200 -0.648996 -0.035710 -0.421195  \n",
      "201 -0.390769  0.345851 -0.253540  \n",
      "202  0.791509  0.169993  0.874288  \n",
      "203  0.273287  0.830436  0.751740  \n",
      "204  0.176448 -1.056699 -0.138072  \n",
      "205 -0.504207 -1.152603 -0.096376  \n",
      "206  1.051040  1.022664 -0.044630  \n",
      "207 -1.048046  0.286884  0.966411  \n",
      "208  0.424238  0.851949 -0.003568  \n",
      "209  0.625308  0.237997 -0.318257  \n",
      "\n",
      "[210 rows x 10 columns]\n",
      "<class 'pandas_ml.core.frame.ModelFrame'>\n",
      "[[ 0.35904303 -0.47803815 -1.19601166 ... -0.4420593   0.79514463\n",
      "   0.68087725]\n",
      " [ 0.62170486 -0.38333035  1.08703928 ... -0.13512565  0.23329268\n",
      "  -0.56830505]\n",
      " [-1.74825501  0.80771031 -0.50822933 ...  0.19957692  0.51306794\n",
      "  -0.77122527]\n",
      " ...\n",
      " [ 2.47268041 -0.73513212 -0.1180798  ... -1.04804639  0.28688362\n",
      "   0.96641075]\n",
      " [ 1.31277984  0.2496128   1.17541159 ...  0.42423805  0.85194912\n",
      "  -0.00356751]\n",
      " [ 2.084072    0.87270593  0.06394634 ...  0.62530761  0.23799661\n",
      "  -0.31825701]]\n",
      "<class 'numpy.ndarray'>\n",
      "15:10:56.162703\n",
      "15:10:56.192720\n",
      "15:10:56.257763\n",
      "15:10:56.293785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 3/150, cost: 0.6103800047537332\n",
      "INFO:pyswarms.single.global_best:Iteration 4/150, cost: 0.6103800047537332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:56.361293\n",
      "15:10:56.397293\n",
      "15:10:56.445295\n",
      "15:10:56.485295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 5/150, cost: 0.6103800047537332\n",
      "INFO:pyswarms.single.global_best:Iteration 6/150, cost: 0.6103800047537332\n",
      "INFO:pyswarms.single.global_best:Iteration 7/150, cost: 0.5854348781304369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:56.597298\n",
      "15:10:56.629368\n",
      "15:10:56.666734\n",
      "15:10:56.686735\n",
      "15:10:56.730903\n",
      "15:10:56.746530\n",
      "15:10:56.798450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 8/150, cost: 0.5854348781304369\n",
      "INFO:pyswarms.single.global_best:Iteration 9/150, cost: 0.5663039802039332\n",
      "INFO:pyswarms.single.global_best:Iteration 10/150, cost: 0.5501397082326843\n",
      "INFO:pyswarms.single.global_best:Iteration 11/150, cost: 0.4391800735016917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:56.830567\n",
      "15:10:56.865142\n",
      "15:10:56.880767\n",
      "15:10:56.920626\n",
      "15:10:56.944628\n",
      "15:10:56.976627\n",
      "15:10:57.004629\n",
      "15:10:57.040630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 12/150, cost: 0.4391800735016917\n",
      "INFO:pyswarms.single.global_best:Iteration 13/150, cost: 0.4391800735016917\n",
      "INFO:pyswarms.single.global_best:Iteration 14/150, cost: 0.4391800735016917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:57.071336\n",
      "15:10:57.100827\n",
      "15:10:57.136748\n",
      "15:10:57.176751\n",
      "15:10:57.212752\n",
      "15:10:57.248752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 15/150, cost: 0.407925434914349\n",
      "INFO:pyswarms.single.global_best:Iteration 16/150, cost: 0.4060543169963739\n",
      "INFO:pyswarms.single.global_best:Iteration 17/150, cost: 0.4060543169963739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:57.284755\n",
      "15:10:57.326332\n",
      "15:10:57.350251\n",
      "15:10:57.396437\n",
      "15:10:57.436439\n",
      "15:10:57.468524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 18/150, cost: 0.4060543169963739\n",
      "INFO:pyswarms.single.global_best:Iteration 19/150, cost: 0.3785081710166399\n",
      "INFO:pyswarms.single.global_best:Iteration 20/150, cost: 0.3785081710166399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:57.504448\n",
      "15:10:57.552447\n",
      "15:10:57.580644\n",
      "15:10:57.612645\n",
      "15:10:57.640647\n",
      "15:10:57.672648\n",
      "15:10:57.700648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 21/150, cost: 0.3785081710166399\n",
      "INFO:pyswarms.single.global_best:Iteration 22/150, cost: 0.3763158235579705\n",
      "INFO:pyswarms.single.global_best:Iteration 23/150, cost: 0.3694404944568653\n",
      "INFO:pyswarms.single.global_best:Iteration 24/150, cost: 0.3534782485616967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:57.736647\n",
      "15:10:57.764648\n",
      "15:10:57.792651\n",
      "15:10:57.816652\n",
      "15:10:57.848650\n",
      "15:10:57.872652\n",
      "15:10:57.904744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 25/150, cost: 0.3534782485616967\n",
      "INFO:pyswarms.single.global_best:Iteration 26/150, cost: 0.3534782485616967\n",
      "INFO:pyswarms.single.global_best:Iteration 27/150, cost: 0.3534782485616967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:57.940531\n",
      "15:10:57.972532\n",
      "15:10:58.008536\n",
      "15:10:58.044534\n",
      "15:10:58.072823\n",
      "15:10:58.111473\n",
      "15:10:58.140270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 28/150, cost: 0.3534782485616967\n",
      "INFO:pyswarms.single.global_best:Iteration 29/150, cost: 0.3445787030034901\n",
      "INFO:pyswarms.single.global_best:Iteration 30/150, cost: 0.3445787030034901\n",
      "INFO:pyswarms.single.global_best:Iteration 31/150, cost: 0.3445787030034901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:58.199131\n",
      "15:10:58.231133\n",
      "15:10:58.291319\n",
      "15:10:58.322570\n",
      "15:10:58.359112\n",
      "15:10:58.391112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 32/150, cost: 0.3445787030034901\n",
      "INFO:pyswarms.single.global_best:Iteration 33/150, cost: 0.3373856120746704\n",
      "INFO:pyswarms.single.global_best:Iteration 34/150, cost: 0.3373856120746704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:58.435115\n",
      "15:10:58.471114\n",
      "15:10:58.516749\n",
      "15:10:58.548887\n",
      "15:10:58.588757\n",
      "15:10:58.616757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 35/150, cost: 0.3373856120746704\n",
      "INFO:pyswarms.single.global_best:Iteration 36/150, cost: 0.3308060964456939\n",
      "INFO:pyswarms.single.global_best:Iteration 37/150, cost: 0.32668737026457134\n",
      "INFO:pyswarms.single.global_best:Iteration 38/150, cost: 0.32668737026457134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:58.668760\n",
      "15:10:58.700759\n",
      "15:10:58.732760\n",
      "15:10:58.756763\n",
      "15:10:58.784761\n",
      "15:10:58.816764\n",
      "15:10:58.848764\n",
      "15:10:58.860813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 39/150, cost: 0.32605100189495173\n",
      "INFO:pyswarms.single.global_best:Iteration 40/150, cost: 0.3247427903658261\n",
      "INFO:pyswarms.single.global_best:Iteration 41/150, cost: 0.3206252020877276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:58.910772\n",
      "15:10:58.938602\n",
      "15:10:58.970603\n",
      "15:10:58.998949\n",
      "15:10:59.040447\n",
      "15:10:59.076449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 42/150, cost: 0.3152613908706718\n",
      "INFO:pyswarms.single.global_best:Iteration 43/150, cost: 0.3152613908706718\n",
      "INFO:pyswarms.single.global_best:Iteration 44/150, cost: 0.31342288308197647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:59.112308\n",
      "15:10:59.152181\n",
      "15:10:59.192181\n",
      "15:10:59.228185\n",
      "15:10:59.265795\n",
      "15:10:59.289793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 45/150, cost: 0.3110024207566155\n",
      "INFO:pyswarms.single.global_best:Iteration 46/150, cost: 0.31034833197091605\n",
      "INFO:pyswarms.single.global_best:Iteration 47/150, cost: 0.31034833197091605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:59.336275\n",
      "15:10:59.368274\n",
      "15:10:59.416276\n",
      "15:10:59.448277\n",
      "15:10:59.496276\n",
      "15:10:59.528278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 48/150, cost: 0.30661248852613854\n",
      "INFO:pyswarms.single.global_best:Iteration 49/150, cost: 0.30637773035878585\n",
      "INFO:pyswarms.single.global_best:Iteration 50/150, cost: 0.30593308191985286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:59.564280\n",
      "15:10:59.600281\n",
      "15:10:59.640492\n",
      "15:10:59.668992\n",
      "15:10:59.709605\n",
      "15:10:59.734003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 51/150, cost: 0.30593308191985286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:10:59.784305\n",
      "15:10:59.819792\n",
      "15:10:59.965711\n",
      "15:10:59.985705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 52/150, cost: 0.30593308191985286\n",
      "INFO:pyswarms.single.global_best:Iteration 53/150, cost: 0.3047629509611702\n",
      "INFO:pyswarms.single.global_best:Iteration 54/150, cost: 0.301907120468134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:00.035617\n",
      "15:11:00.095593\n",
      "15:11:00.127594\n",
      "15:11:00.183595\n",
      "15:11:00.219490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 55/150, cost: 0.3007515110794316\n",
      "INFO:pyswarms.single.global_best:Iteration 56/150, cost: 0.30014192455177113\n",
      "INFO:pyswarms.single.global_best:Iteration 57/150, cost: 0.2992573313496384\n",
      "INFO:pyswarms.single.global_best:Iteration 58/150, cost: 0.2985501401771796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:00.255312\n",
      "15:11:00.291312\n",
      "15:11:00.323313\n",
      "15:11:00.355481\n",
      "15:11:00.386733\n",
      "15:11:00.422349\n",
      "15:11:00.437977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 59/150, cost: 0.2960670482017815\n",
      "INFO:pyswarms.single.global_best:Iteration 60/150, cost: 0.2939232704491144\n",
      "INFO:pyswarms.single.global_best:Iteration 61/150, cost: 0.29301200833280255\n",
      "INFO:pyswarms.single.global_best:Iteration 62/150, cost: 0.2925142769521338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:00.480994\n",
      "15:11:00.504991\n",
      "15:11:00.540994\n",
      "15:11:00.564992\n",
      "15:11:00.588904\n",
      "15:11:00.604532\n",
      "15:11:00.640435\n",
      "15:11:00.675874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 63/150, cost: 0.29222419104932845\n",
      "INFO:pyswarms.single.global_best:Iteration 64/150, cost: 0.2918248166832291\n",
      "INFO:pyswarms.single.global_best:Iteration 65/150, cost: 0.2906704930999644\n",
      "INFO:pyswarms.single.global_best:Iteration 66/150, cost: 0.2906704930999644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:00.708071\n",
      "15:11:00.739325\n",
      "15:11:00.768203\n",
      "15:11:00.792203\n",
      "15:11:00.808682\n",
      "15:11:00.839932\n",
      "15:11:00.875462\n",
      "15:11:00.891090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 67/150, cost: 0.2888712015853878\n",
      "INFO:pyswarms.single.global_best:Iteration 68/150, cost: 0.28718464128948534\n",
      "INFO:pyswarms.single.global_best:Iteration 69/150, cost: 0.2847068854377364\n",
      "INFO:pyswarms.single.global_best:Iteration 70/150, cost: 0.28329635181474333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:00.938978\n",
      "15:11:00.962978\n",
      "15:11:00.997726\n",
      "15:11:01.017727\n",
      "15:11:01.051624\n",
      "15:11:01.075625\n",
      "15:11:01.094170\n",
      "15:11:01.125420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 71/150, cost: 0.2812054004357766\n",
      "INFO:pyswarms.single.global_best:Iteration 72/150, cost: 0.27936881439585876\n",
      "INFO:pyswarms.single.global_best:Iteration 73/150, cost: 0.2773860543081716\n",
      "INFO:pyswarms.single.global_best:Iteration 74/150, cost: 0.27521425841424974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:01.164094\n",
      "15:11:01.188096\n",
      "15:11:01.212160\n",
      "15:11:01.240325\n",
      "15:11:01.272322\n",
      "15:11:01.292324\n",
      "15:11:01.324324\n",
      "15:11:01.348610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 75/150, cost: 0.27297609083811436\n",
      "INFO:pyswarms.single.global_best:Iteration 76/150, cost: 0.2712480135418906\n",
      "INFO:pyswarms.single.global_best:Iteration 77/150, cost: 0.2694430166275589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:01.380611\n",
      "15:11:01.404613\n",
      "15:11:01.428712\n",
      "15:11:01.460198\n",
      "15:11:01.480504\n",
      "15:11:01.524755\n",
      "15:11:01.556754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 78/150, cost: 0.2672613350541344\n",
      "INFO:pyswarms.single.global_best:Iteration 79/150, cost: 0.26599621203178553\n",
      "INFO:pyswarms.single.global_best:Iteration 80/150, cost: 0.26310414872949195\n",
      "INFO:pyswarms.single.global_best:Iteration 81/150, cost: 0.2609088104788491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:01.584762\n",
      "15:11:01.608755\n",
      "15:11:01.640757\n",
      "15:11:01.672760\n",
      "15:11:01.696760\n",
      "15:11:01.724816\n",
      "15:11:01.752814\n",
      "15:11:01.780926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 82/150, cost: 0.2583954017426919\n",
      "INFO:pyswarms.single.global_best:Iteration 83/150, cost: 0.2556445451993844\n",
      "INFO:pyswarms.single.global_best:Iteration 84/150, cost: 0.25392748518611025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:01.823221\n",
      "15:11:01.867220\n",
      "15:11:01.903657\n",
      "15:11:01.931531\n",
      "15:11:01.971035\n",
      "15:11:02.003035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 85/150, cost: 0.2530329841476762\n",
      "INFO:pyswarms.single.global_best:Iteration 86/150, cost: 0.2515945578164456\n",
      "INFO:pyswarms.single.global_best:Iteration 87/150, cost: 0.2509428578936206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:02.047038\n",
      "15:11:02.082876\n",
      "15:11:02.123324\n",
      "15:11:02.160585\n",
      "15:11:02.200587\n",
      "15:11:02.240587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 88/150, cost: 0.24978519594421544\n",
      "INFO:pyswarms.single.global_best:Iteration 89/150, cost: 0.24950021296496314\n",
      "INFO:pyswarms.single.global_best:Iteration 90/150, cost: 0.2489271883614498\n",
      "INFO:pyswarms.single.global_best:Iteration 91/150, cost: 0.24762619340560216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:02.288588\n",
      "15:11:02.324589\n",
      "15:11:02.356592\n",
      "15:11:02.385057\n",
      "15:11:02.416553\n",
      "15:11:02.456973\n",
      "15:11:02.483569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 92/150, cost: 0.24640664308319812\n",
      "INFO:pyswarms.single.global_best:Iteration 93/150, cost: 0.24453101673002703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:02.539059\n",
      "15:11:02.571117\n",
      "15:11:02.603416\n",
      "15:11:02.736025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 94/150, cost: 0.2434113313811302\n",
      "INFO:pyswarms.single.global_best:Iteration 95/150, cost: 0.2423982874457296\n",
      "INFO:pyswarms.single.global_best:Iteration 96/150, cost: 0.24159262497279604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:02.782903\n",
      "15:11:02.818865\n",
      "15:11:02.854870\n",
      "15:11:02.878866\n",
      "15:11:02.918866\n",
      "15:11:02.950868\n",
      "15:11:02.990871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 97/150, cost: 0.24120912002227352\n",
      "INFO:pyswarms.single.global_best:Iteration 98/150, cost: 0.24018005360449993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15:11:03.030872\n",
      "15:11:03.091959\n",
      "15:11:03.127983\n",
      "15:11:03.168007\n",
      "15:11:03.193026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 99/150, cost: 0.23898284994477234\n",
      "INFO:pyswarms.single.global_best:Iteration 100/150, cost: 0.2385745785139866\n",
      "INFO:pyswarms.single.global_best:Iteration 101/150, cost: 0.23785144722664503\n",
      "INFO:pyswarms.single.global_best:Iteration 102/150, cost: 0.23669915730134017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:03.238055\n",
      "15:11:03.270079\n",
      "15:11:03.311630\n",
      "15:11:03.343632\n",
      "15:11:03.375836\n",
      "15:11:03.399801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 103/150, cost: 0.2355563595636933\n",
      "INFO:pyswarms.single.global_best:Iteration 104/150, cost: 0.23526808238972674\n",
      "INFO:pyswarms.single.global_best:Iteration 105/150, cost: 0.23372944357872272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:03.452933\n",
      "15:11:03.477091\n",
      "15:11:03.517432\n",
      "15:11:03.541432\n",
      "15:11:03.574861\n",
      "15:11:03.602863\n",
      "15:11:03.627070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 106/150, cost: 0.2329367303211093\n",
      "INFO:pyswarms.single.global_best:Iteration 107/150, cost: 0.2317676905798753\n",
      "INFO:pyswarms.single.global_best:Iteration 108/150, cost: 0.23114769327540943\n",
      "INFO:pyswarms.single.global_best:Iteration 109/150, cost: 0.2305388950712602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:03.662555\n",
      "15:11:03.690554\n",
      "15:11:03.708882\n",
      "15:11:03.752869\n",
      "15:11:03.776871\n",
      "15:11:03.804267\n",
      "15:11:03.828193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 110/150, cost: 0.2301168583866946\n",
      "INFO:pyswarms.single.global_best:Iteration 111/150, cost: 0.2292180153022474\n",
      "INFO:pyswarms.single.global_best:Iteration 112/150, cost: 0.2284236322089237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:03.869817\n",
      "15:11:03.901999\n",
      "15:11:03.938001\n",
      "15:11:03.962001\n",
      "15:11:03.998347\n",
      "15:11:04.018347\n",
      "15:11:04.046413\n",
      "15:11:04.062044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 113/150, cost: 0.22743781730834675\n",
      "INFO:pyswarms.single.global_best:Iteration 114/150, cost: 0.22674373033809067\n",
      "INFO:pyswarms.single.global_best:Iteration 115/150, cost: 0.2259697089031704\n",
      "INFO:pyswarms.single.global_best:Iteration 116/150, cost: 0.2248285665634912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "15:11:04.109876\n",
      "15:11:04.141876\n",
      "15:11:04.177877\n",
      "15:11:04.201877\n",
      "15:11:04.241879\n",
      "15:11:04.269881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 117/150, cost: 0.22390274575126973\n",
      "INFO:pyswarms.single.global_best:Iteration 118/150, cost: 0.22324072958883145\n",
      "INFO:pyswarms.single.global_best:Iteration 119/150, cost: 0.22250607157453262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:04.305880\n",
      "15:11:04.337881\n",
      "15:11:04.366037\n",
      "15:11:04.401920\n",
      "15:11:04.433833\n",
      "15:11:04.449458\n",
      "15:11:04.502744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 120/150, cost: 0.2216889217681172\n",
      "INFO:pyswarms.single.global_best:Iteration 121/150, cost: 0.2206726827349776\n",
      "INFO:pyswarms.single.global_best:Iteration 122/150, cost: 0.21946320711705922\n",
      "INFO:pyswarms.single.global_best:Iteration 123/150, cost: 0.2184228687405824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:04.538743\n",
      "15:11:04.562743\n",
      "15:11:04.594745\n",
      "15:11:04.634765\n",
      "15:11:04.666135\n",
      "15:11:04.702461\n",
      "15:11:04.734666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 124/150, cost: 0.21758452203372575\n",
      "INFO:pyswarms.single.global_best:Iteration 125/150, cost: 0.21666437670821895\n",
      "INFO:pyswarms.single.global_best:Iteration 126/150, cost: 0.21582556999237063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:04.774670\n",
      "15:11:04.802670\n",
      "15:11:04.834671\n",
      "15:11:04.858754\n",
      "15:11:04.890755\n",
      "15:11:04.914755\n",
      "15:11:04.946755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 127/150, cost: 0.2150481614248783\n",
      "INFO:pyswarms.single.global_best:Iteration 128/150, cost: 0.21404672205680517\n",
      "INFO:pyswarms.single.global_best:Iteration 129/150, cost: 0.21289291740756153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:04.978757\n",
      "15:11:05.004801\n",
      "15:11:05.052648\n",
      "15:11:05.088650\n",
      "15:11:05.116650\n",
      "15:11:05.156651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 130/150, cost: 0.21199335940498115\n",
      "INFO:pyswarms.single.global_best:Iteration 131/150, cost: 0.21129777929361548\n",
      "INFO:pyswarms.single.global_best:Iteration 132/150, cost: 0.2105744772977012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:05.189094\n",
      "15:11:05.242382\n",
      "15:11:05.274383\n",
      "15:11:05.310384\n",
      "15:11:05.330384\n",
      "15:11:05.370385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 133/150, cost: 0.2097695561517437\n",
      "INFO:pyswarms.single.global_best:Iteration 134/150, cost: 0.20899361171032896\n",
      "INFO:pyswarms.single.global_best:Iteration 135/150, cost: 0.20812750249406742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:05.394386\n",
      "15:11:05.435679\n",
      "15:11:05.471741\n",
      "15:11:05.503743\n",
      "15:11:05.539745\n",
      "15:11:05.579746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 136/150, cost: 0.20736580801115043\n",
      "INFO:pyswarms.single.global_best:Iteration 137/150, cost: 0.20668427554997668\n",
      "INFO:pyswarms.single.global_best:Iteration 138/150, cost: 0.20577177666065546\n",
      "INFO:pyswarms.single.global_best:Iteration 139/150, cost: 0.20488725188093732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:05.608067\n",
      "15:11:05.646842\n",
      "15:11:05.674841\n",
      "15:11:05.710845\n",
      "15:11:05.730844\n",
      "15:11:05.762846\n",
      "15:11:05.790845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 140/150, cost: 0.20405020172208926\n",
      "INFO:pyswarms.single.global_best:Iteration 141/150, cost: 0.2032985334295719\n",
      "INFO:pyswarms.single.global_best:Iteration 142/150, cost: 0.20218125670150836\n",
      "INFO:pyswarms.single.global_best:Iteration 143/150, cost: 0.20122239999843047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:05.826847\n",
      "15:11:05.850847\n",
      "15:11:05.882849\n",
      "15:11:05.910850\n",
      "15:11:05.938849\n",
      "15:11:05.958694\n",
      "15:11:05.998982\n",
      "15:11:06.018981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 144/150, cost: 0.20045463270645916\n",
      "INFO:pyswarms.single.global_best:Iteration 145/150, cost: 0.19984032600869894\n",
      "INFO:pyswarms.single.global_best:Iteration 146/150, cost: 0.19922618894690264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:06.056044\n",
      "15:11:06.075736\n",
      "15:11:06.105232\n",
      "15:11:06.129234\n",
      "15:11:06.161811\n",
      "15:11:06.201354\n",
      "15:11:06.237357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 147/150, cost: 0.19835248523612842\n",
      "INFO:pyswarms.single.global_best:Iteration 148/150, cost: 0.19765734842140695\n",
      "INFO:pyswarms.single.global_best:Iteration 149/150, cost: 0.19710682866501655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:06.273357\n",
      "15:11:06.309461\n",
      "15:11:06.341049\n",
      "15:11:06.377051\n",
      "15:11:06.409051\n",
      "15:11:06.449052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyswarms.single.global_best:Iteration 150/150, cost: 0.196530801884009\n",
      "INFO:pyswarms.single.global_best:================================\n",
      "Optimization finished!\n",
      "Final cost: 0.1965\n",
      "Best value: [ 0.684013 -0.481605 0.943835 ...]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:11:06.485057\n"
     ]
    }
   ],
   "source": [
    "# Wrapper With  Ann PSO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import datetime\n",
    "import pandas_ml as pdml\n",
    "import imblearn\n",
    "# Import PySwarms\n",
    "import pyswarms as ps\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "df = pd.read_excel('Health.xlsx')\n",
    "#df = df.drop(['Time','Amount'],axis=1)\n",
    "#X = df.iloc[:,:-1]\n",
    "#y = df['Class']\n",
    "\n",
    "X=Health[['sen','jens','dard ghafase sineh','feshar khun dar halat esterahat','kolestrol','ghand khun nashta','navar ghalb dar halat esterahat','hadaksar zaraban ghalb','anjin sadri nashi az varzesh','afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat','shibe tamrin dar oje tamrin dar maghtae ST','talasemi']]\n",
    "y=Health['Outcome']\n",
    "\n",
    "# number_records_fraud = len(df[df.Class == 1])\n",
    "# fraud_indices = np.array(df[df.Class == 1].index)\n",
    "\n",
    "# # Picking the indices of the normal classes\n",
    "# normal_indices = df[df.Class == 0].index\n",
    "\n",
    "# # Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "# random_normal_indices = np.random.choice(normal_indices, 3*number_records_fraud, replace = False)\n",
    "# random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# # Appending the 2 indices\n",
    "# under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# # Under sample dataset\n",
    "# under_sample_data = df.iloc[under_sample_indices,:]\n",
    "\n",
    "# X = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n",
    "# y = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = scale(X)\n",
    "pca = PCA(n_components=10)\n",
    "X = pca.fit_transform(data)\n",
    "print(X)\n",
    "print(y)\n",
    "print(type(X))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "df2 = pdml.ModelFrame(X_train, target=y_train)\n",
    "sampler = df2.imbalance.over_sampling.SMOTE()\n",
    "oversampled = df2.fit_sample(sampler)\n",
    "X, y = oversampled.iloc[:,1:11], oversampled['Outcome']\n",
    "\n",
    "print(X)\n",
    "print(type(X))\n",
    "X=X.as_matrix()\n",
    "y=y.as_matrix()\n",
    "print(X)\n",
    "print(type(X))\n",
    "\n",
    "def forward_prop(params):\n",
    "    \"\"\"Forward propagation as objective function\n",
    "\n",
    "    This computes for the forward propagation of the neural network, as\n",
    "    well as the loss. It receives a set of parameters that must be\n",
    "    rolled-back into the corresponding weights and biases.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    params: np.ndarray\n",
    "        The dimensions should include an unrolled version of the\n",
    "        weights and biases.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The computed negative log-likelihood loss given the parameters\n",
    "    \"\"\"\n",
    "    # Neural network architecture\n",
    "    n_inputs = 10\n",
    "    n_hidden = 20\n",
    "    n_classes = 2\n",
    "\n",
    "    # Roll-back the weights and biases\n",
    "    W1 = params[0:200].reshape((n_inputs,n_hidden))\n",
    "    b1 = params[200:220].reshape((n_hidden,))\n",
    "    W2 = params[220:260].reshape((n_hidden,n_classes))\n",
    "    b2 = params[260:262].reshape((n_classes,))\n",
    "\n",
    "    #print(W1)\n",
    "    #print(W2)\n",
    "\n",
    "    # Perform forward propagation\n",
    "    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n",
    "    logits = z2          # Logits for Layer 2\n",
    "\n",
    "    # Compute for the softmax of the logits\n",
    "    exp_scores = np.exp(logits)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute for the negative log likelihood\n",
    "    N = len(X) # Number of samples\n",
    "    corect_logprobs = -np.log(probs[range(N), y])\n",
    "    loss = np.sum(corect_logprobs) / N\n",
    "    return loss\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Higher-level method to do forward_prop in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    n_particles = x.shape[0]\n",
    "    j = [forward_prop(x[i]) for i in range(n_particles)]\n",
    "    print(datetime.datetime.now().time())\n",
    "    return np.array(j)\n",
    "\n",
    "# Initialize swarm\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
    "\n",
    "# Call instance of PSO\n",
    "dimensions = 262\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=50, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(f, print_step=1, iters=150, verbose=3)\n",
    "\n",
    "#Pass X, pos to check for training set and X_test, pos for testing set\n",
    "def predict(X, pos):\n",
    "    \"\"\"\n",
    "    Use the trained weights to perform class predictions.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X: numpy.ndarray\n",
    "        Input Iris dataset\n",
    "    pos: numpy.ndarray\n",
    "        Position matrix found by the swarm. Will be rolled\n",
    "        into weights and biases.\n",
    "    \"\"\"\n",
    "    # Neural network architecture\n",
    "    n_inputs = 10\n",
    "    n_hidden = 20\n",
    "    n_classes = 2\n",
    "\n",
    "    # Roll-back the weights and biases\n",
    "    W1 = pos[0:200].reshape((n_inputs,n_hidden))\n",
    "    b1 = pos[200:220].reshape((n_hidden,))\n",
    "    W2 = pos[220:260].reshape((n_hidden,n_classes))\n",
    "    b2 = pos[260:262].reshape((n_classes,))\n",
    "\n",
    "    # Perform forward propagation\n",
    "    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n",
    "    a1 = np.tanh(z1)     # Activation in Layer 1\n",
    "    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n",
    "    logits = z2          # Logits for Layer 2\n",
    "\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return y_pred\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0   -2.072338 -0.514916 -0.817271  1.433877  0.024120 -0.051764  1.148120   \n",
      "1   -2.043423  0.407313  0.180833 -1.451351 -1.911490 -0.483255  0.418618   \n",
      "2    0.950051 -1.167152 -0.174613 -0.836656 -0.861086  0.715580 -0.370840   \n",
      "3   -1.839257  1.867578  0.319770 -0.552813  1.610644 -0.045007  1.919947   \n",
      "4    0.353275 -0.394382  0.695202 -0.242671 -0.776290  0.344571 -0.997895   \n",
      "5    1.061348  0.637276  1.330675 -0.084828  1.894457  0.525315  1.188849   \n",
      "6   -0.749926 -0.631619  0.538986 -0.431694  0.738131 -1.013629 -1.307867   \n",
      "7    3.095636 -0.205889 -1.198876  0.943325 -0.437292  1.788749 -0.565546   \n",
      "8   -0.280838  1.095579 -1.420075  0.438591 -0.630783  0.913863  0.297022   \n",
      "9    1.910425 -0.458222 -0.976467 -1.144529  0.185119 -1.610757  0.414467   \n",
      "10   2.145921  0.896405  0.657891 -2.621226  0.637254  0.646418  0.928789   \n",
      "11   1.943687 -0.803409  0.171641  0.765346  1.079487 -0.653916 -0.960133   \n",
      "12  -0.354493  0.580389  0.395002  0.653265 -1.606722  0.405995 -0.732468   \n",
      "13   0.304746  2.855474 -1.687830 -0.301953 -1.191939 -0.521176  0.395315   \n",
      "14   2.847462 -0.599446 -0.600755 -0.392467  1.264232 -0.895141 -0.503722   \n",
      "15   2.007336 -2.726570 -0.106416  1.622707 -0.282907 -0.096586  1.219335   \n",
      "16  -1.591942 -0.153662 -0.701722  1.451517  0.177933 -0.307182  1.071026   \n",
      "17   1.660521 -1.181519 -0.839045 -0.488005 -0.188156 -1.330837  1.068156   \n",
      "18   2.091612 -1.068103 -0.666019 -0.578680  0.077749 -1.476547  0.727343   \n",
      "19   0.711455 -0.744874  0.653594  0.385896 -1.407831  0.221813 -0.602565   \n",
      "20  -2.667950  0.559747 -0.911372  0.294204 -0.097410 -1.031331  0.792784   \n",
      "21  -0.994420 -1.778076  1.647876 -0.025722 -0.366456 -1.711066  0.335874   \n",
      "22   0.589855  0.137774  0.865295  1.429529 -1.050976  0.746490 -0.868075   \n",
      "23   0.798588  0.877429 -1.356017  0.782658  0.932285 -0.665621  0.727880   \n",
      "24   2.177493 -1.063846 -1.103883 -0.461129 -0.221894 -1.355464  0.624252   \n",
      "25   0.368458  1.156770 -1.135231 -2.204875  0.129159  0.655979 -1.296751   \n",
      "26   1.873204 -0.288155  0.578079  0.165870 -1.082423 -0.248255 -0.358648   \n",
      "27  -2.091380  0.427674  0.331638 -0.975758 -1.623846 -0.030291 -0.332241   \n",
      "28  -2.237754 -0.607548  0.091954 -0.440267  0.050873 -0.746902  0.007441   \n",
      "29  -0.702837 -1.503357  0.932560  0.909149  0.207725 -0.506145 -0.345953   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "182 -1.233886  1.738824  0.110894  1.694411  1.032140 -0.803567 -1.258687   \n",
      "183  3.048144 -0.986797 -0.426097  0.293861 -0.174010 -1.696693  0.693073   \n",
      "184  0.055617 -1.455137 -0.480182 -1.814782  0.732989 -1.064964  0.181130   \n",
      "185  1.193344  1.967501 -0.397495  0.940191  2.213989  0.736258  1.442012   \n",
      "186 -2.108171 -0.276132  0.439459  0.033186 -0.082598 -0.925610 -0.338350   \n",
      "187 -0.050352 -1.502729 -0.488854 -2.474467 -1.328768 -0.197634  0.927315   \n",
      "188  0.186778  0.833121 -0.780172 -0.574221 -0.696886  0.848519 -0.935133   \n",
      "189 -0.578249 -1.537530  0.211274 -0.634293  0.546159 -0.907576 -0.279887   \n",
      "190  0.657880  0.073964 -0.382077  1.898536 -1.306735  0.935162  1.369272   \n",
      "191  1.239353 -0.452679 -0.307365 -1.818282 -1.249314 -0.468648  0.613735   \n",
      "192 -0.671863  1.706475  0.908145 -0.465459  0.103214  0.372028 -2.367702   \n",
      "193 -0.431670 -0.101411 -2.145504  0.635146 -0.922137  1.654658  0.775351   \n",
      "194  0.805954 -0.698022  0.594793  0.744191  0.503264 -0.937012 -0.899154   \n",
      "195 -3.013211 -1.791375  0.847647  0.571082  0.053964  0.027447 -0.111876   \n",
      "196 -1.825254  0.423898 -1.557290 -0.123767  0.678556 -0.555137  0.684954   \n",
      "197 -2.046670 -1.292407 -0.666461  2.107494  0.239046  0.648306  0.943716   \n",
      "198  2.804692 -0.672648 -0.313872 -0.399284 -0.908198  0.128528 -0.146411   \n",
      "199  1.323817  1.348578  0.108251  0.178543  1.704908 -0.787609  1.407646   \n",
      "200 -0.716535 -0.692258  0.560819 -0.315468  0.653941 -0.998585 -1.219904   \n",
      "201 -0.942359  0.231406  0.105275 -0.692980  0.695681 -1.112169 -1.162849   \n",
      "202 -1.337796 -0.647930 -0.828551 -1.387193 -1.088056  0.884042 -0.040591   \n",
      "203  1.256478 -0.476214 -0.316471 -1.788768 -1.229271 -0.427967  0.584868   \n",
      "204 -0.029571 -0.101322  0.480802 -1.227351 -0.981752 -0.390906 -0.769655   \n",
      "205  0.575912 -0.137486  1.065898 -0.162157 -1.110454 -0.417738 -0.943130   \n",
      "206  3.512206 -0.623641  0.353912  1.844434  0.581087 -0.389473 -0.408114   \n",
      "207 -0.063204 -1.201390 -0.157683 -1.532858  0.910392 -1.058788 -0.207783   \n",
      "208  2.999949 -0.358988 -0.907860  0.502036 -0.596276  1.239496 -0.422875   \n",
      "209 -0.222516  0.591352  1.721561 -0.805499 -1.228750 -1.255479 -0.842253   \n",
      "210 -0.046068 -0.026589  0.958138  2.095960 -0.154801 -0.767011 -0.688250   \n",
      "211  1.806003 -0.314573  0.582447  0.178597 -1.101246 -0.221065 -0.372757   \n",
      "\n",
      "            7         8         9  \n",
      "0   -0.337704 -0.309960 -0.671057  \n",
      "1   -0.200681 -0.034739  0.517465  \n",
      "2    0.648453  0.736367 -0.928867  \n",
      "3    0.694956 -0.179969  0.123127  \n",
      "4    0.291923 -0.726504 -1.034505  \n",
      "5   -0.065871  1.465231  1.026519  \n",
      "6    1.079477  0.223212 -0.760214  \n",
      "7    0.419656  0.963479  0.709878  \n",
      "8   -0.337360 -0.808362 -0.475843  \n",
      "9    0.991161  0.677875 -0.054219  \n",
      "10   1.082845  0.320147  0.388589  \n",
      "11  -0.059533 -1.012633 -0.081565  \n",
      "12   1.195729  0.395882  0.706129  \n",
      "13   0.774878 -0.720002  0.331098  \n",
      "14  -0.296185  0.821204 -0.411723  \n",
      "15  -0.374266  0.185928  0.197747  \n",
      "16  -0.828381 -0.099577 -0.805135  \n",
      "17   0.249085  0.806660  0.198596  \n",
      "18   0.219815  0.672155 -0.091076  \n",
      "19   0.975193 -1.023399 -0.105374  \n",
      "20   0.434197  0.655492 -0.563092  \n",
      "21  -0.851143  1.899668 -0.621108  \n",
      "22  -0.620132 -0.543498  1.358919  \n",
      "23  -1.355638  0.558244  0.522816  \n",
      "24   1.709549  0.385448  0.693761  \n",
      "25   0.076839  0.727142  1.223108  \n",
      "26  -0.457601 -1.285927 -0.065509  \n",
      "27   0.492048  0.618801  0.200589  \n",
      "28  -0.069842 -0.074995  0.804650  \n",
      "29   0.040877  0.392886 -1.245002  \n",
      "..        ...       ...       ...  \n",
      "182  0.158369  0.899197  0.194844  \n",
      "183  0.337173  0.395601  0.952835  \n",
      "184 -0.389830  0.344012 -0.242412  \n",
      "185  1.067787 -0.545331  0.517624  \n",
      "186  0.414066  0.583505  0.769043  \n",
      "187 -0.627732  0.399565 -0.834687  \n",
      "188  0.977508 -0.668243  0.779679  \n",
      "189 -0.087262 -1.389717  0.107966  \n",
      "190 -1.860997  0.897543 -0.710042  \n",
      "191 -0.449388  0.460548 -0.947641  \n",
      "192 -0.597096  0.533361  0.392215  \n",
      "193  0.590728 -0.592185 -0.807374  \n",
      "194  0.756951 -0.686959 -0.093945  \n",
      "195 -0.563993  0.326070  0.275481  \n",
      "196 -0.424513 -0.731206  0.065363  \n",
      "197 -0.530106 -0.553621 -0.523628  \n",
      "198  0.060501  0.145844  0.229346  \n",
      "199 -1.504587  0.141460 -0.099160  \n",
      "200  1.107049  0.203946 -0.780717  \n",
      "201  0.935819  0.208399  0.159371  \n",
      "202  0.356904 -0.630295  0.365918  \n",
      "203 -0.423996  0.450420 -0.932260  \n",
      "204  0.245099 -1.223788  0.093104  \n",
      "205  0.040997 -1.087086  0.451438  \n",
      "206 -0.794515 -0.432551  0.203930  \n",
      "207 -0.542849 -0.170939 -0.213263  \n",
      "208  0.300888  0.695718  0.552837  \n",
      "209 -0.825389 -0.738772  0.034064  \n",
      "210  0.522988  0.692080  1.203537  \n",
      "211 -0.374722 -1.270741 -0.067815  \n",
      "\n",
      "[212 rows x 10 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "5      0\n",
      "6      1\n",
      "7      1\n",
      "8      0\n",
      "9      1\n",
      "10     1\n",
      "11     1\n",
      "12     1\n",
      "13     0\n",
      "14     1\n",
      "15     1\n",
      "16     0\n",
      "17     1\n",
      "18     1\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     1\n",
      "23     1\n",
      "24     1\n",
      "25     1\n",
      "26     1\n",
      "27     0\n",
      "28     0\n",
      "29     0\n",
      "      ..\n",
      "182    0\n",
      "183    1\n",
      "184    1\n",
      "185    1\n",
      "186    0\n",
      "187    1\n",
      "188    0\n",
      "189    1\n",
      "190    0\n",
      "191    1\n",
      "192    0\n",
      "193    0\n",
      "194    0\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    1\n",
      "199    1\n",
      "200    1\n",
      "201    1\n",
      "202    1\n",
      "203    1\n",
      "204    1\n",
      "205    1\n",
      "206    1\n",
      "207    1\n",
      "208    1\n",
      "209    1\n",
      "210    1\n",
      "211    1\n",
      "Name: Outcome, Length: 212, dtype: int64\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 27)                297       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                560       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,188\n",
      "Trainable params: 1,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 212 samples, validate on 99 samples\n",
      "Epoch 1/5\n",
      "212/212 [==============================] - 2s 8ms/step - loss: 0.7335 - acc: 0.5377 - val_loss: 0.7142 - val_acc: 0.5455\n",
      "Epoch 2/5\n",
      "212/212 [==============================] - 0s 113us/step - loss: 0.7131 - acc: 0.5330 - val_loss: 0.6919 - val_acc: 0.6061\n",
      "Epoch 3/5\n",
      "212/212 [==============================] - 0s 132us/step - loss: 0.6949 - acc: 0.5755 - val_loss: 0.6729 - val_acc: 0.6364\n",
      "Epoch 4/5\n",
      "212/212 [==============================] - 0s 132us/step - loss: 0.6794 - acc: 0.5896 - val_loss: 0.6574 - val_acc: 0.6364\n",
      "Epoch 5/5\n",
      "212/212 [==============================] - 0s 132us/step - loss: 0.6671 - acc: 0.6226 - val_loss: 0.6447 - val_acc: 0.6869\n",
      "Loss:  [0.6446972040816991, 0.6868686868686869]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200f5341438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  0.3939393939393939\n",
      "FP:  0.25252525252525254\n",
      "TN:  0.29292929292929293\n",
      "FN:  0.06060606060606061\n",
      "Accuracy:  0.6868686868686869\n",
      "Recall:  0.8666666666666667\n",
      "Precision:  0.609375\n"
     ]
    }
   ],
   "source": [
    "#annSGD\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Flatten, Dropout, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas_ml as pdml\n",
    "import imblearn\n",
    "\n",
    "df = pd.read_excel('Health.xlsx')\n",
    "X=Health[['sen','jens','dard ghafase sineh','feshar khun dar halat esterahat','kolestrol','ghand khun nashta','navar ghalb dar halat esterahat','hadaksar zaraban ghalb','anjin sadri nashi az varzesh','afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat','shibe tamrin dar oje tamrin dar maghtae ST','talasemi']]\n",
    "y=Health['Outcome']\n",
    "\n",
    "df.head()\n",
    "\n",
    "from pandas_ml import ConfusionMatrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = scale(X)\n",
    "pca = PCA(n_components=10)\n",
    "X = pca.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "df = pdml.ModelFrame(X_train, target=y_train)\n",
    "sampler = df.imbalance.over_sampling.SMOTE()\n",
    "oversampled = df.fit_sample(sampler)\n",
    "X, y = oversampled.iloc[:,1:11], oversampled['Outcome']\n",
    "print(X)\n",
    "print(y)\n",
    "X=X.as_matrix()\n",
    "y=y.as_matrix()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(27, input_dim=10, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "h = model.fit(X, y, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"Loss: \", model.evaluate(X_test, y_test, verbose=2))\n",
    "y_predicted = np.round(model.predict(X_test)).T[0]\n",
    "y_correct = np.array(y_test)\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(y_correct, y_predicted)\n",
    "confusion_matrix.plot(normalized=True)\n",
    "plt.show()\n",
    "#confusion_matrix2.print_stats()\n",
    "\n",
    "false_neg=0.0\n",
    "false_pos=0.0\n",
    "true_pos=0.0\n",
    "true_neg=0.0\n",
    "incorrect=0.0\n",
    "total=len(y_predicted)\n",
    "\n",
    "for i in range(len(y_predicted)):\n",
    "\tif y_predicted[i]!=y_correct[i] :\n",
    "\t\tincorrect+=1\n",
    "\t\tif y_predicted[i] == 1 and y_correct[i] == 0 :\n",
    "\t\t\tfalse_pos+=1\n",
    "\t\telse :\n",
    "\t\t\tfalse_neg+=1\n",
    "\telse :\n",
    "\t\tif y_predicted[i] == 1 and y_correct[i] == 1 :\n",
    "\t\t\ttrue_pos+=1\n",
    "\t\telse :\n",
    "\t\t\ttrue_neg+=1\n",
    "print(\"TP: \", true_pos/total)\n",
    "print(\"FP: \", false_pos/total)\n",
    "print(\"TN: \", true_neg/total)\n",
    "print(\"FN: \", false_neg/total)\n",
    "inaccuracy = incorrect/total\n",
    "accuracy = 1 - inaccuracy\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "recall = 0.0\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "precision = true_pos/(true_pos + false_pos )\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows 10\\AppData\\Roaming\\Python\\Python36\\site-packages\\matplotlib\\axes\\_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200f7851f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5234899328859061\n",
      "False negative rate (with respect to misclassifications):  0.8309859154929577\n",
      "False negative rate (with respect to all the data):  0.3959731543624161\n",
      "False negatives, false positives, mispredictions: 59 12 71\n",
      "Total test data points: 149\n"
     ]
    }
   ],
   "source": [
    "#k_means\n",
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_excel('Health.xlsx')\n",
    "\n",
    "X=Health[['sen','jens','dard ghafase sineh','feshar khun dar halat esterahat','kolestrol','ghand khun nashta','navar ghalb dar halat esterahat','hadaksar zaraban ghalb','anjin sadri nashi az varzesh','afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat','shibe tamrin dar oje tamrin dar maghtae ST','talasemi']]\n",
    "y=Health['Outcome']\n",
    "\n",
    "X_scaled = scale(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size = 0.5)\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=2, n_init=100)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Step size of the mesh.\n",
    "h = .01\n",
    "\n",
    "# Plot the decision boundary and assign a color to each\n",
    "#x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
    "#y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
    "#xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot Result\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(X_reduced[:, 0], X_reduced[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend()\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "predictions = kmeans.predict(X_test)\n",
    "\n",
    "spred_fraud = np.where(predictions == 1)[0]\n",
    "real_fraud = np.where(y_test == 1)[0]\n",
    "false_pos = len(np.setdiff1d(spred_fraud, real_fraud))\n",
    "\n",
    "pred_good = np.where(predictions == 0)[0]\n",
    "real_good = np.where(y_test == 0)[0]\n",
    "false_neg = len(np.setdiff1d(spred_fraud, real_good))\n",
    "\n",
    "false_neg_rate = false_neg*1.0/(false_pos+false_neg)\n",
    "accuracy = 0.0\n",
    "accuracy = (len(X_test) - (false_neg + false_pos)) / (len(X_test)*1.0)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"False negative rate (with respect to misclassifications): \", false_neg_rate)\n",
    "print(\"False negative rate (with respect to all the data): \", false_neg*1.0 / len(predictions))\n",
    "print(\"False negatives, false positives, mispredictions:\", false_neg, false_pos, false_neg + false_pos)\n",
    "print(\"Total test data points:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6  \\\n",
      "0   -1.198919 -0.417791  0.955239  0.686363  0.645033 -0.610687 -0.751140   \n",
      "1    1.120785 -1.091626 -0.503981 -1.269311  1.006655 -1.021362 -0.341310   \n",
      "2    0.186778  0.833121 -0.780172 -0.574221 -0.696886  0.848519 -0.935133   \n",
      "3    2.438157 -1.020851 -0.147659 -0.368733 -0.572271  0.550223 -0.339213   \n",
      "4   -0.329830  2.366489 -0.688386  0.460835 -0.301890  0.403797 -0.636221   \n",
      "5   -3.013211 -1.791375  0.847647  0.571082  0.053964  0.027447 -0.111876   \n",
      "6   -1.765038 -0.607185  0.726494  0.612692  0.480759 -0.357055 -0.849122   \n",
      "7    0.108139  1.813997 -1.502423 -0.337519  0.657882  1.266850 -1.464416   \n",
      "8    2.441968 -0.748193  0.375689 -0.474477 -0.378958  0.042580 -0.715680   \n",
      "9   -1.435881 -1.290632 -0.184165 -0.735547  0.027099 -0.655282  0.705879   \n",
      "10  -1.839257  1.867578  0.319770 -0.552813  1.610644 -0.045007  1.919947   \n",
      "11   2.827335 -0.688576 -0.376320 -0.329929 -0.349851  0.667523 -0.791204   \n",
      "12   2.616603 -1.417652 -1.091447 -0.191426  0.730804 -0.548271 -0.011586   \n",
      "13   1.943253 -2.309618 -0.088709  0.501046  1.515095  0.376296 -0.565743   \n",
      "14  -1.684572  0.785162 -0.056340  1.241308 -1.541722  0.458608  0.600467   \n",
      "15  -1.979922 -1.241311 -0.547175 -0.228213  0.387423  0.138194 -0.299961   \n",
      "16  -1.179607 -0.551591  0.109191 -0.194987 -0.088457 -1.184347  0.501252   \n",
      "17   0.693285  1.665160 -0.775157  1.818460  1.363071  0.966014 -1.618129   \n",
      "18  -1.967643 -1.487896  0.189727  0.222667  0.568957  0.125207 -0.262603   \n",
      "19   1.889123 -2.720482 -0.407982  0.035762 -0.854067  1.409490  0.382691   \n",
      "20  -0.420036  0.899338  3.314938  0.119354  0.392962 -1.493693  1.267586   \n",
      "21  -0.671863  1.706475  0.908145 -0.465459  0.103214  0.372028 -2.367702   \n",
      "22  -2.093313 -0.437442  0.454340  0.270902  0.177261 -0.526523 -0.713113   \n",
      "23  -0.243218  0.174933  1.095692  2.187425 -0.001794 -0.642031 -1.136061   \n",
      "24   1.388289  1.334067  3.443278  1.500490 -0.424248  0.831680  0.313830   \n",
      "25  -2.132138  0.291856 -0.674961  1.089758  0.324917 -0.486649  0.204406   \n",
      "26  -1.477549  0.959142  1.147330 -0.448249 -0.637000  0.158134 -1.388465   \n",
      "27   4.332949  2.761993  1.647773  0.727749 -0.147642 -0.745927  2.700152   \n",
      "28   2.523999  1.792872  1.171788  0.025318  0.562302  0.515185  2.165479   \n",
      "29  -1.859988 -1.283475  0.745773 -0.671606 -0.160375 -1.389172 -0.183026   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "194  0.028651  0.430324 -1.497400 -0.156230 -0.742780  0.809126  1.227144   \n",
      "195 -1.191236  1.608337 -1.631705 -0.884525 -0.427061  0.452887 -0.005136   \n",
      "196  0.513863  0.325194  2.006222  1.142955 -0.262392  2.245529  1.281436   \n",
      "197 -1.233886  1.738824  0.110894  1.694411  1.032140 -0.803567 -1.258687   \n",
      "198  2.773203 -0.006133 -0.022868 -0.696539 -1.001656 -0.490329 -0.098886   \n",
      "199  0.363049 -1.782432  0.620054  1.046174  0.184444 -0.560977 -0.280964   \n",
      "200 -1.259298 -0.679552 -0.880935 -1.347330 -0.751721  0.671595 -0.152723   \n",
      "201  2.094194 -0.237219 -1.251864  0.129834 -1.005423  0.793416  0.224761   \n",
      "202  2.557774 -0.723915  0.155683 -0.450966 -0.552465  0.065926 -0.528710   \n",
      "203  2.331305  0.069420  0.151698  0.096816 -0.579324  0.156582 -1.131543   \n",
      "204 -0.019213 -1.474879 -0.489613 -1.738822  0.754187 -0.982082  0.089916   \n",
      "205  2.673743 -1.190808 -0.906314 -0.373701  0.107832 -0.097135 -0.098005   \n",
      "206  2.434805 -1.013869 -0.149059 -0.379790 -0.567325  0.546295 -0.343462   \n",
      "207  3.115853  0.960007  0.342547  3.077314 -1.295279 -0.239940  0.400636   \n",
      "208  0.327828  0.062010 -2.064214  0.710673  0.880917  0.143835  0.749603   \n",
      "209  0.106497 -1.816608  1.251796  1.071889 -1.107582 -1.680215  0.863963   \n",
      "210 -0.615131 -1.386542 -0.274048 -0.994966  1.046814 -0.430068 -0.846708   \n",
      "211 -0.123383 -1.502363 -0.502743 -1.633079  0.783698 -0.866701 -0.037062   \n",
      "212  2.025318 -0.573461  0.179805  0.614006  0.672271 -0.470280 -1.010002   \n",
      "213  1.155948 -1.081603 -0.505952 -1.251454  1.011901 -1.018792 -0.344617   \n",
      "214  0.373365 -0.315862  1.082233 -0.095483 -1.168990 -0.501386 -0.750421   \n",
      "215  4.014277  2.591268  1.563922  0.604006 -0.022575 -0.523765  2.605962   \n",
      "216  3.561630  0.077073  0.869143 -0.497568  0.240496  1.505006  1.533987   \n",
      "217  1.294732  0.257408  1.063882 -1.090879  0.297691  1.722394  1.408005   \n",
      "218  1.685031  0.104303  0.962727 -0.628650  0.108809  1.841292  1.764607   \n",
      "219  3.095918  0.088445  0.233841  0.380122  0.932462 -1.485691 -0.897440   \n",
      "220  1.205908 -0.597588 -1.593609 -0.658616 -0.750785  1.496441 -0.038747   \n",
      "221  0.091952 -0.774643  0.054132 -1.036801 -1.283983  0.014560 -0.023429   \n",
      "222  2.832326 -0.730289 -0.799095 -0.227155 -0.627721  0.800868 -0.296066   \n",
      "223 -1.421528  0.979028  1.185895 -0.436807 -0.657573  0.104438 -1.389613   \n",
      "\n",
      "            7         8         9  \n",
      "0   -1.337775 -0.434871  1.499541  \n",
      "1   -0.168226  0.310785  0.331247  \n",
      "2    0.977508 -0.668243  0.779679  \n",
      "3   -0.440075  0.101314 -0.151953  \n",
      "4   -0.371990 -0.262654 -0.562982  \n",
      "5   -0.563993  0.326070  0.275481  \n",
      "6   -0.264818  0.493317  0.712089  \n",
      "7    0.139568  2.031157 -0.612518  \n",
      "8   -1.269355 -0.233427  1.038598  \n",
      "9   -0.886828  1.349887  0.912770  \n",
      "10   0.694956 -0.179969  0.123127  \n",
      "11   0.170491  0.063641  0.139788  \n",
      "12   0.778700  0.742232 -0.173391  \n",
      "13  -1.080019  0.257686 -0.503277  \n",
      "14  -0.428398  0.144197 -0.883967  \n",
      "15   0.734223  0.114679  0.528263  \n",
      "16  -1.010990 -0.753275  1.605553  \n",
      "17  -0.412969 -0.250912 -0.207460  \n",
      "18  -0.768615  0.005003  0.394823  \n",
      "19  -0.543732  0.237170 -0.780729  \n",
      "20   1.420503  0.506074 -0.513736  \n",
      "21  -0.597096  0.533361  0.392215  \n",
      "22   0.571268  0.397584  0.939482  \n",
      "23   0.914688  1.081595  0.970956  \n",
      "24   1.292192  0.387800 -0.508021  \n",
      "25   0.258076  0.368011 -0.094757  \n",
      "26  -0.974611  0.410511  0.306933  \n",
      "27  -1.667809 -0.156398 -0.248809  \n",
      "28  -1.670708 -0.165377 -0.167539  \n",
      "29   0.855547  0.074964 -0.867895  \n",
      "..        ...       ...       ...  \n",
      "194 -1.616448  0.632492 -0.668694  \n",
      "195 -0.164420 -0.811192 -0.099319  \n",
      "196  0.535014 -0.392857  0.314958  \n",
      "197  0.158369  0.899197  0.194844  \n",
      "198 -0.322712  0.127745  0.326371  \n",
      "199  0.434392 -0.673584 -0.409243  \n",
      "200  0.581197 -0.655828  0.382214  \n",
      "201  0.306581  0.863338  1.409311  \n",
      "202 -0.844981 -0.111526  0.780529  \n",
      "203  0.440225 -1.223570  0.219804  \n",
      "204 -0.243727  0.194726 -0.246168  \n",
      "205  0.541681  0.601398 -0.233649  \n",
      "206 -0.443635  0.104258 -0.153778  \n",
      "207 -1.058779 -1.979205  0.464300  \n",
      "208 -0.081516  0.546157  0.359159  \n",
      "209  0.429250  1.085502 -0.141954  \n",
      "210  0.588168 -1.233507 -0.245831  \n",
      "211 -0.040337 -0.013096 -0.251396  \n",
      "212  0.068087 -1.095369 -0.006640  \n",
      "213 -0.170832  0.321179  0.316116  \n",
      "214 -0.068050 -1.085396  0.650094  \n",
      "215 -1.668320 -0.157980 -0.234492  \n",
      "216  1.055580  0.417531 -0.647298  \n",
      "217  0.385768  0.813803  0.194158  \n",
      "218  0.122544  0.434554  0.767783  \n",
      "219 -0.040083  0.912672  0.468218  \n",
      "220  0.664076  1.038815  0.552886  \n",
      "221  0.353794 -1.291839 -0.644077  \n",
      "222  0.282745  0.554019 -0.060705  \n",
      "223 -0.944554  0.417465  0.246496  \n",
      "\n",
      "[224 rows x 10 columns]\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "5      0\n",
      "6      0\n",
      "7      0\n",
      "8      1\n",
      "9      0\n",
      "10     0\n",
      "11     1\n",
      "12     1\n",
      "13     1\n",
      "14     0\n",
      "15     1\n",
      "16     0\n",
      "17     0\n",
      "18     0\n",
      "19     1\n",
      "20     0\n",
      "21     0\n",
      "22     0\n",
      "23     1\n",
      "24     0\n",
      "25     0\n",
      "26     1\n",
      "27     1\n",
      "28     1\n",
      "29     0\n",
      "      ..\n",
      "194    0\n",
      "195    0\n",
      "196    0\n",
      "197    0\n",
      "198    1\n",
      "199    1\n",
      "200    1\n",
      "201    1\n",
      "202    1\n",
      "203    1\n",
      "204    1\n",
      "205    1\n",
      "206    1\n",
      "207    1\n",
      "208    1\n",
      "209    1\n",
      "210    1\n",
      "211    1\n",
      "212    1\n",
      "213    1\n",
      "214    1\n",
      "215    1\n",
      "216    1\n",
      "217    1\n",
      "218    1\n",
      "219    1\n",
      "220    1\n",
      "221    1\n",
      "222    1\n",
      "223    1\n",
      "Name: Outcome, Length: 224, dtype: int64\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 27)                297       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                560       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,188\n",
      "Trainable params: 1,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 224 samples, validate on 99 samples\n",
      "Epoch 1/5\n",
      "224/224 [==============================] - 1s 2ms/step - loss: 0.6947 - acc: 0.5089 - val_loss: 0.6845 - val_acc: 0.4747\n",
      "Epoch 2/5\n",
      "224/224 [==============================] - 0s 163us/step - loss: 0.6614 - acc: 0.5714 - val_loss: 0.6575 - val_acc: 0.5455\n",
      "Epoch 3/5\n",
      "224/224 [==============================] - 0s 147us/step - loss: 0.6301 - acc: 0.6741 - val_loss: 0.6305 - val_acc: 0.6263\n",
      "Epoch 4/5\n",
      "224/224 [==============================] - 0s 161us/step - loss: 0.5944 - acc: 0.7321 - val_loss: 0.6012 - val_acc: 0.6667\n",
      "Epoch 5/5\n",
      "224/224 [==============================] - 0s 125us/step - loss: 0.5569 - acc: 0.7812 - val_loss: 0.5690 - val_acc: 0.7273\n",
      "Loss:  [0.5689707278001188, 0.7272727278747944]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200f53410f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  0.30303030303030304\n",
      "FP:  0.06060606060606061\n",
      "TN:  0.42424242424242425\n",
      "FN:  0.21212121212121213\n",
      "Accuracy:  0.7272727272727273\n",
      "Recall:  0.5882352941176471\n",
      "Precision:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#annAdadelta\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Flatten, Dropout, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas_ml as pdml\n",
    "import imblearn\n",
    "\n",
    "df = pd.read_excel('Health.xlsx')\n",
    "X=Health[['sen','jens','dard ghafase sineh','feshar khun dar halat esterahat','kolestrol','ghand khun nashta','navar ghalb dar halat esterahat','hadaksar zaraban ghalb','anjin sadri nashi az varzesh','afsordegi st nashi az tamrin va varzesh nesbat be halat esterahat','shibe tamrin dar oje tamrin dar maghtae ST','talasemi']]\n",
    "y=Health['Outcome']\n",
    "\n",
    "df.head()\n",
    "\n",
    "from pandas_ml import ConfusionMatrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data = scale(X)\n",
    "pca = PCA(n_components=10)\n",
    "X = pca.fit_transform(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "df = pdml.ModelFrame(X_train, target=y_train)\n",
    "sampler = df.imbalance.over_sampling.SMOTE()\n",
    "oversampled = df.fit_sample(sampler)\n",
    "X, y = oversampled.iloc[:,1:11], oversampled['Outcome']\n",
    "print(X)\n",
    "print(y)\n",
    "X=X.as_matrix()\n",
    "y=y.as_matrix()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(27, input_dim=10, activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "h = model.fit(X, y, epochs=5, validation_data=(X_test, y_test))\n",
    "\n",
    "print(\"Loss: \", model.evaluate(X_test, y_test, verbose=2))\n",
    "y_predicted = np.round(model.predict(X_test)).T[0]\n",
    "y_correct = np.array(y_test)\n",
    "\n",
    "confusion_matrix = ConfusionMatrix(y_correct, y_predicted)\n",
    "confusion_matrix.plot(normalized=True)\n",
    "plt.show()\n",
    "#confusion_matrix2.print_stats()\n",
    "\n",
    "false_neg=0.0\n",
    "false_pos=0.0\n",
    "true_pos=0.0\n",
    "true_neg=0.0\n",
    "incorrect=0.0\n",
    "total=len(y_predicted)\n",
    "\n",
    "for i in range(len(y_predicted)):\n",
    "\tif y_predicted[i]!=y_correct[i] :\n",
    "\t\tincorrect+=1\n",
    "\t\tif y_predicted[i] == 1 and y_correct[i] == 0 :\n",
    "\t\t\tfalse_pos+=1\n",
    "\t\telse :\n",
    "\t\t\tfalse_neg+=1\n",
    "\telse :\n",
    "\t\tif y_predicted[i] == 1 and y_correct[i] == 1 :\n",
    "\t\t\ttrue_pos+=1\n",
    "\t\telse :\n",
    "\t\t\ttrue_neg+=1\n",
    "print(\"TP: \", true_pos/total)\n",
    "print(\"FP: \", false_pos/total)\n",
    "print(\"TN: \", true_neg/total)\n",
    "print(\"FN: \", false_neg/total)\n",
    "inaccuracy = incorrect/total\n",
    "accuracy = 1 - inaccuracy\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "recall = 0.0\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "precision = true_pos/(true_pos + false_pos )\n",
    "\n",
    "print(\"Recall: \", recall)\n",
    "print(\"Precision: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
